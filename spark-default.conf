#Runtime Environment
spark.executor.extraJavaOptions		-XX:+PrintGCDetails -XX:+PrintGCTimeStamps

#Shuffle Behavior
spark.reducer.maxSizeInFlight		128m 	#48m  Maximum size of map outputs to fetch simultaneously from each reduce task. Since each output requires us to create a buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory. 
spark.shuffle.compress			true 	#true Whether to compress map output files. Generally a good idea. Compression will use spark.io.compression.codec. 
spark.shuffle.manager			SORT 	#sort Implementation to use for shuffling data. There are two implementations available: sort and hash. Sort-based shuffle is more memory-efficient and is the default option starting in 1.2. 
spark.shuffle.spill.compress		true 	#true Whether to compress data spilled during shuffles. Compression will use spark.io.compression.codec. 

#Spark UI
spark.eventLog.compress			false 	#false  Whether to compress logged events, if spark.eventLog.enabled is true. 
spark.eventLog.enabled			true 	#false 	Whether to log Spark events, useful for reconstructing the Web UI after the application has finished. 
spark.eventLog.dir			file:///home/dyagoubi/scratch/  #Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server. 

#Compression and Serialization

spark.io.compression.codec		org.apache.spark.io.LZ4CompressionCodec 	#snappy   The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs. By default, Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec
spark.io.compression.lz4.blockSize 	32k  	#Block size used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used. 
spark.serializer                   	org.apache.spark.serializer.KryoSerializer  	#Class to use for serializing objects that will be sent over the network or need to be cached in serialized form. The default of Java serialization works with any Serializable Java object but is quite slow, so we recommend using org.apache.spark.serializer.KryoSerializer and configuring Kryo serialization when speed is necessary. Can be any subclass of org.apache.spark.Serializer. 
spark.kryo.registrationRequired 	true 	#false Whether to require registration with Kryo. If set to 'true', Kryo will throw an exception if an unregistered class is serialized. If set to false (the default), Kryo will write unregistered class names along with each object. Writing class names can cause significant performance overhead, so enabling this option can enforce strictly that a user has not omitted classes from registration. 
spark.kryoserializer.buffer.max 	128m 	#Maximum allowable size of Kryo serialization buffer. This must be larger than any object you attempt to serialize. Increase this if you get a "buffer limit exceeded" exception inside Kryo. 
spark.kryoserializer.buffer 		64k 	#Initial size of Kryo's serialization buffer. Note that there will be one buffer per core on each worker. This buffer will grow up to spark.kryoserializer.buffer.max if needed. 
spark.rdd.compress 			true 	#false Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time. 

#Memory Management

spark.memory.fraction 			0.75 	#Fraction of (heap space - 300MB) used for execution and storage. The lower this is, the more frequently spills and cached data eviction occur. The purpose of this config is to set aside memory for internal metadata, user data structures, and imprecise size estimation in the case of sparse, unusually large records. Leaving this at the default value is recommended. For more detail, see this description. 
spark.memory.storageFraction 		0.5 	#Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by sâ€‹park.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended.

#Execution Behavior



#spark.dynamicAllocation.enabled    true
spark.local.dir file:///home/dyagoubi/scratch/
